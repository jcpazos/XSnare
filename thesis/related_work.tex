\section{Related Work}
In the following sections, we discuss a number of related works and how they compare with our own. We are primarily interested in the distinction between different techniques: client-side, server-side, and a combination of these; and how they can be used in tandem with our approach.
\subsection{Server-side techniques} In addition to existing parameter sanitization techniques, taint-tracking has been proposed as a means to consolidate sanitization of vulnerable parameters \cite{Xu:2006:TPE:1267336.1267345,DBLP:conf/sec/Nguyen-TuongGGSE05,Pietraszek:2005:DAI:2146257.2146267,Bisht:2008:XPD:1428322.1428325}. These techniques are complementary to ours and, provide an additional line of defence against XSS. However, many of them rely on the client-side rendering to maintain the server-side properties, which will not always be the case.

\subsection{Client-side techniques} There has been previous work in client-side defenses against XSS, our work is not novel in this respect. Noxes \cite{Kirda:2009:CCS:2639535.2639808} presents a similar approach as a client-side firewall-based network proxy. Rules dictate the links which can be accessed by a website when generating requests, and can be created both automatically and manually by an user. This technique does not protect against same-service attacks, such as code deleting local files. Furthermore, they rely on websites having a small amount of external dynamic links to third-parties. This likely does not hold true anymore, as websites require an ever-increasing amount of dynamic content, with several interconnections with third-parties, such as advertisement, analytics, and other user interactions.

 DOMPurify  \cite{10.1007/978-3-319-66399-9_7} presents a robust XSS filter that works purely on the client-side. The authors argue that the DOM is the ideal place for sanitization to occur. While we agree with this view, their work relies on application developers to adopt their filter and modify their code to use it. This is a problem because developers might not be aware of vulnerable points in their application beforehand. In our study, we saw many instances of input parameters lacking basic sanitization. Thus, this technique is complementary to ours, and we have decided to use the DOMPurify filter for our injection points. We also believe the API is straightforward and simple to use, and won't require much signature developer effort to use effectively.
 
 Jim et al. \cite{Jim:2007:DSI:1242572.1242654} present a method to defend against injection attacks through Browser-Enforced Embedded Policies. This approach is similar to ours, as the policies specify places where script execution should not occur. However, policies are defined by the application developers, and this again relies on them to know where their code might be vulnerable. Furthermore, browser modifications are required to benefit from it, and issues of cross-portability and backwards compatibility arise.

Although not solely related to XSS, Snyder et al. \cite{Snyder:2017:MWD:3133956.3133966} report a study in which they disable several JavaScript APIs and tests the amount of websites that are clearly non-functional without the full functionality of the APIs. They present a novel technique to interpose on JavaScript execution via the use of ES6 Proxies, allowing for efficient trapping of function calls. This approach increases security due to the number of vulnerabilities present in several JavaScript APIs, however, we believe disabling whole aspects of API functionality should only be used as a last resort.

Browsers are equipped several built-in browser defences:
\begin{itemize}
	\item  URL filters, such as Chrome's XSS auditor attempt to protect from certain attacks like reflected XSS. Unfortunately, these filters are often plagued by false positives and bypasses.
	\item Content Security Policy (CSP) has been widely adopted and in many cases provides developers with a reliable way to protect against XSS and Cross-site request forgery (CSRF) attacks. However, CSP requires the developer to know which scripts might be malicious. This is particularly hard in the case of inline scripts, like the ones used in many XSS exploits.
	\item Same-origin policy is another useful security mechanism for protection against XSS and CSRF. This policy restricts how a document or script loaded from one origin can interact with a resource from another origin. This is useful in many attack scenarios, particularly against CSRF in cross-origin attacks. As with CSP, if the attack is injected in the same website the attacker intends to compromise, this will not defend against it.
\end{itemize}

As with other approaches these browser defences are complementary to ours.

\subsection{Client and server hybrids}
Nadji et al. \cite{Nadji:2009} make use of a hybrid approach to XSS defences. They use sever-specified policies that are enforced on the client-side. Unlike previous work, they do not rely on developers to identified untrusted sources, and tag elements server-side, such that the client-side has a clear distinction of untrusted code and can filter it accordingly. Our own tagging mechanism is partly inspired by this, but we do not rely on the server passing this information along, and thus is less effective. However, as previously mentioned, the adoption of server-side techniques might not be feasible for many developers.

