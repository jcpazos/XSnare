\section{Related Work}
In the following sections, we discuss related work and how it compares with our own. We highlight the similarities and how our work improves upon these previous ideas. We classify existing work into multiple categories: client-side, server-side, browser built-in, and hybrid: a combination of these.

\subsection{Server-side techniques} In addition to existing parameter sanitization techniques, taint-tracking has been proposed as a means to consolidate sanitization of vulnerable parameters \cite{Xu:2006:TPE:1267336.1267345,DBLP:conf/sec/Nguyen-TuongGGSE05,Pietraszek:2005:DAI:2146257.2146267,Bisht:2008:XPD:1428322.1428325}. These techniques are complementary to ours and, provide an additional line of defence against XSS. However, many of them rely on the client-side rendering to maintain the server-side properties, which will not always be the case.

Furthermore, these defences do not protect against purely client-side XSS, such as reflected XSS, or persistent client-side XSS, which uses a browser's local storage or cookies as an attack vector. Steffens et al. \cite{DBLP:conf/ndss/SteffensRJS19} present a study of Persistent Client-Side XSS across popular websites and find that as many as 21\% of the most frequented web sites are vulnerable to these attacks.  While some of these exploits are harder to carry out in practice, as many as 6\% of these sites are vulnerable to an attack by a realistic attacker.

Our framework is not particular to client or server-side XSS as long as the exploit string appears in the HTML of the web site.

\subsection{Client-side techniques} There has been previous work in client-side defenses against XSS, our work is not novel in this respect. Noxes \cite{Kirda:2009:CCS:2639535.2639808} presents a similar approach as a client-side firewall-based network proxy. Rules dictate the links which can be accessed by a website when generating requests, and can be created both automatically and manually by an user. This technique does not protect against same-service attacks, such as code deleting local files. Furthermore, they rely on websites having a small amount of external dynamic links to third-parties. This likely does not hold true anymore, as websites require an ever-increasing amount of dynamic content, with several interconnections with third-parties, such as advertisement, analytics, and other user interactions.

DOMPurify  \cite{10.1007/978-3-319-66399-9_7} presents a robust XSS filter that works purely on the client-side. The authors argue that the DOM is the ideal place for sanitization to occur. While we agree with this view, their work relies on application developers to adopt their filter and modify their code to use it. This is a problem because developers might not be aware of vulnerable points in their application beforehand. In our study, we saw many instances of input parameters lacking basic sanitization. Thus, this technique is complementary to ours, and we have decided to use the DOMPurify filter for our injection points. We also believe the API is straightforward and simple to use, and won't require much signature developer effort to use effectively.

Jim et al. \cite{Jim:2007:DSI:1242572.1242654} present a method to defend against injection attacks through Browser-Enforced Embedded Policies. This approach is similar to ours, as the policies specify places where script execution should not occur. However, policies are defined by the application developers, and this again relies on them to know where their code might be vulnerable. Furthermore, browser modifications are required to benefit from it, and issues of cross-portability and backwards compatibility arise.

Hallaraker and Vigna \cite{Hallaraker:2005:DMJ:1078029.1078861} use a policy language to detect malicious code on the client-side. Similarly to us, they make use of signatures to protect against known types of exploits. However, unlike our approach, their signatures are not application-level, and there is no model for signature maintenance. Furthermore, there is no evaluation on the efficacy of their signatures. 

Although not solely related to XSS, Snyder et al. \cite{Snyder:2017:MWD:3133956.3133966} report a study in which they disable several JavaScript APIs and tests the amount of websites that are clearly non-functional without the full functionality of the APIs. They present a novel technique to interpose on JavaScript execution via the use of ES6 Proxies, allowing for efficient trapping of function calls. This approach increases security due to the number of vulnerabilities present in several JavaScript APIs, however, we believe disabling whole aspects of API functionality should only be used as a last resort.

\subsection{Browser built-in defences}
Browsers are equipped with several built-in defences:
\begin{itemize}
	\item  URL filters, such as Chrome's XSS auditor attempt to protect from certain attacks like reflected XSS. Unfortunately, these filters are often plagued by false positives and bypasses.
	\item Content Security Policy (CSP) \cite{CSP} has been widely adopted and in many cases provides developers with a reliable way to protect against XSS and Cross-site request forgery (CSRF) attacks. However, CSP requires the developer to know which scripts might be malicious. This is particularly hard in the case of inline scripts, like the ones used in many XSS exploits.
	\item Same-origin policy \cite{SOP} is another useful security mechanism for protection against XSS and CSRF. This policy restricts how a document or script loaded from one origin can interact with a resource from another origin. This is useful in many attack scenarios, particularly against CSRF in cross-origin attacks. As with CSP, if the attack is injected in the same website the attacker intends to compromise, this will not defend against it.
\end{itemize}

As with other approaches these browser defences are complementary to ours.

\subsection{Client and server hybrids}
Nadji et al. \cite{Nadji:2009} make use of a hybrid approach to XSS defences. They use sever-specified policies that are enforced on the client-side. Unlike previous work, they do not rely on developers to identify untrusted sources, and tag elements server-side, such that the client-side has a clear distinction of untrusted code and can filter it accordingly. Our own tagging mechanism is partly inspired by this, as injection point markers are identified based on surrounding elements, but we do not rely on the server passing this information along, and thus is less precise.
 
XSS-Dec \cite{10.1007/978-3-642-31540-4_17} also employs a hybrid solution, mainly via the use of a proxy, which keeps track of an encrypted version of the server's source files, and uses this information to derive exploits in a page visited by the user. This approach is similar to ours, in the sense that we assume previous knowledge of the clean HTML document. Furthermore, they use both anomaly-based detection and signature-based detection to calculate the likelihood of an attack taking place, and prevent it from happening. However, there is no mention of signature maintenance. In a way, our system offloads all this functionality to the client-side, without the need of any server-side information.

