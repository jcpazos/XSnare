\section{Related Work}
We classify existing work into several categories: client-side, server-side, browser built-in, and hybrid approaches.

\textbf{Server-side techniques.}
In addition to existing parameter sanitization techniques,
taint-tracking has been proposed as a means to consolidate
sanitization of vulnerable parameters, and identify vulnerabilities automatically. ~\cite{10.1145/1368088.1368112, Xu:2006:TPE:1267336.1267345,DBLP:conf/sec/Nguyen-TuongGGSE05,Pietraszek:2005:DAI:2146257.2146267,Bisht:2008:XPD:1428322.1428325, 5070521}. These techniques are complementary to ours, and provide an additional line
of defence against \ac{XSS}.
% However, many of them rely on the
%client-side rendering to maintain the server-side properties, which
%will not always be the case.

There has also been work on other server-side analysis approaches
to find bugs security vulnerabilities in web applications. \cite{10.1145/1390630.1390661, 5416728, 10.1145/2393596.2393608}.
However, these do not target XSS specifically.

\textbf{Client-side techniques.}
DOMPurify~\cite{10.1007/978-3-319-66399-9_7} presents a robust
\ac{XSS} client-side filter. The authors
argue that the DOM is the ideal place for sanitization to occur. While
we agree with this view, this work relies on application developers
to adopt the filter and modify their code to use it. We have partly automated this step by including it as our default sanitization function.

Jim et al.~\cite{Jim:2007:DSI:1242572.1242654} present a method to
defend against injection attacks through Browser-Enforced Embedded
Policies. This approach is similar to ours, as the policies specify
prohibited script execution points. Similarly, Hallaraker and Vigna~\cite{Hallaraker:2005:DMJ:1078029.1078861} use a
policy language to detect malicious code on the client-side. Like \sys, they make use of signatures to protect against known types of
exploits. However, unlike our approach, their signatures are not
application-specific, and there is no model for signature
maintenance.

Snyder et al.~\cite{Snyder:2017:MWD:3133956.3133966} report a study in which
they disable several JavaScript APIs and test the number of websites
that are do not work without the full functionality of the APIs. This approach increases security due to vulnerabilities present in several
JavaScript APIs, however, we believe disabling API functionality
should only be used as a last resort.

Additionally, client-side taint tracking, through the use of static and dynamic analysis,
 has also been applied as a means to detect XSS, either
at the browser level or at the extension level \cite{8094406, 10.1007/978-3-642-04444-1_33}.

\textbf{Browser built-in defences.}  Browsers are equipped
with several built-in defences. We previously described XSS
Auditor in \autoref{introduction}. Another important one is the 
 \ac{CSP}~\cite{CSP}. It has been widely adopted and
in many cases provides developers with a reliable way to protect
against \ac{XSS} and \ac{CSRF} attacks. However, \ac{CSP} requires the developer to identify which scripts
might be malicious. Previous work has also highlighted the need for further built-in defences \cite{6825636}.

\textbf{Client and server hybrids.}
XSS-Dec~\cite{Sundareswaran:2012:XHS:2352970.2352994} uses a proxy which keeps track of an encrypted version of the server's source files, and applies this information to derive exploits in a page visited by the user. This approach is similar to ours, since we assume previous
knowledge of the clean HTML document. Furthermore, they use anomaly-based and signature-based detection to prevent attacks. Our system offloads all this functionality to the client-side, without the need for any server-side information.

