%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Load time performance on top websites} \label{performance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sys's performance goal is to provide its security guarantees without
impacting the user's browsing experience. We now briefly report \sys's
impact on top website load times, representing the expected behaviour
of a user's average web browsing experience.
% For more performance
%evaluation results please see ~\autoref{appendix:perf-eval}.

%  \label{top_sites}
%% We first report our extension's impact on top website load times,
%% representing the expected behaviour of an user's average web browsing
%% experience. 
% 
In our setup, we used a headless version of Firefox 69.0, and
Selenium WebDriver for NodeJS, with GeckoDriver. All experiments were
run on one machine with an Intel Xeon CPU E5-2407 2.40GHz processor,
32 GB DRAM, and our university's 1GiB connection.

In our tests we used the top 500 websites as
reported by Moz.com~\cite{top500}. For each website, we loaded it 25
times (with a 25 second timeout) and recorded the following values:
requestStart, responseEnd, domComplete, and decodedBodySize. From the
initial set of 500, we only report values for 441: the other 59 had
consistent issues with timeouts, insecure certificates, and network
errors. We believe these to have been caused by the Selenium web driver,
as our extension runs after a response has been
delivered to the browser. We manually loaded each page
on a personal computer with our extension running successfully and 
were not able to reproduce the issues.

We ran four test suites:
\textbf{No extension cold cache}: Firefox is loaded without the extension installed and the web driver is re-instantiated for every page load.
\textbf{Extension cold cache}: As before, but Firefox is loaded with the extension installed.
\textbf{No extension warm cache}: Firefox is loaded without the extension installed and the same web driver is used for the page's 25 loads.
\textbf{Extension warm cache}: As before, but Firefox is launched with the extension installed.

For each set of tests, we reduced the recorded values to two comparisons: network filter (responseEnd - requestStart), and page ready (domComplete - responseStart). The first analyzes the time spent by the network filter, while the second determines the time spent until the whole document has loaded. We calculate the medians for each website for each of these measures as well as the decodedByteSize.

\begin{figure}[h]
		\begin{center}
	\includegraphics[scale=0.5]{results/extension_slowdown_overall_small.pdf}
	\caption{Cumulative distribution of relative percentage slowdown with extension installed for top sites.}
	\label{fig:overall_slowdown}
\end{center}
	
\end{figure}

We compare the load times with/without the extension by calculating the relative slowdown with the extension installed according to the following formula:

\begin{equation*}
100*\frac{\tilde{x}_{with}-\tilde{x}_{without}}{\tilde{x}_{without}}
\end{equation*}
\\
where $\tilde{x}$ is the median with/without the extension running.

\autoref{fig:overall_slowdown} plots the results. The graph
shows a slowdown of less than 10\% for 72.6\% of sites, and less than
50\% for 82\% of sites when the extension is running. Note that these
values are recorded as percentages, and while some are as high as
50\%, the absolute values are in 77\% of cases less than a
second. This overhead should not alter the user's experience
significantly.

The slowdown increases by at most 5\% when we take caching into
account. This is likely because the network filter causes the browser
to use less caching, especially for the DOM component, as it might
have to process it from scratch every time. While it may seem
counter-intuitive that some pages have shorter loading times with
the extension, there are several variables at play that can affect
these measurements (local network, server-side load, internal
scheduling, etc). We manually checked the websites for which values
were higher than |40\%| and verified that our extension did not change
the page's contents, a possible cause of faster load times. We also
checked the timings for the page as reported by the browser and noted
a high variance even within small time windows. The time spent by our
verification function was less than 10ms for 87.6\% of sites
(\autoref{fig:verification_time_string_length}). This corroborates our
findings that the slowdown is mostly negligible.

\begin{figure}[h]
	\includegraphics[scale=0.5]{results/string_length_vs_verification_time_small.pdf}
	\caption{Scatter plot of network filter time as a function of character length for top sites.}
	\label{fig:verification_time_string_length}
\end{figure}

\autoref{fig:verification_time_string_length} shows the time spent by the call to our string verification function in the network filter as a function of the length of the string to be verified, differentiating between websites for which some probes tested positive and ones which no probes did. We applied least squares regression to calculate the shown trend lines. 
%The Spearman's rank \footnote{The Spearman's correlation coefficient measures the strength and direction of association between two ranked variables: https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php} correlation values for no probe, probe, and overall are 0.91, 0.91, and 0.72 respectively, demonstrating positive correlation.
 Since both our probes and signatures use regex matching, we expect both trend lines to be linear, as seen in the graph. We expect the slope of the line to be higher when a probe passes, as it performs additional string verification. Around 37.4\% of all web sites use frameworks covered by our probes~\cite{w3stats}, thus, we expect the impact of our network filter to be closer to the non-probe values, as corroborated by our overall trend line.

\textbf{False positives on the Web.} For each website, we recorded the number of loaded signatures. We report a 0\% FP rate for these. Thus, we can infer with confidence that the rate of falsely loaded signatures during an average user's web browsing is similarly low. This rate could possibly go up as we cover more frameworks. Since many of these pages are not running WordPress and are very popular and more prone to fixing their vulnerabilities, the rate of false negatives is likely extremely low as well.
%% Furthermore, since we are using a limited set of signatures and framework 
%% probes,

\textbf{Scalability with signatures.} We tested our system with a large number of signatures. We added
15,500 signatures to our database and recorded the time spent by the
network filter to process these sites\footnote{There are 15,303 CVEs
  related to XSS in CVE Details~\cite{xsscves}.}. These
were crafted so that the extension would check each one against the
loaded sites, without triggering the injection search and
sanitization. Thus, we effectively forced our
extension to test each site against 15,500 signatures. The mean time
spent by the filtering process was 1,930ms, with less than 2,000ms for
88\% of the sites. In practice, we expect a smaller filter time,
as many frameworks would have many signatures. For example, there are
currently 200+ CVEs listed for WordPress core and its plugins.


